[
{
	"uri": "//localhost:1313/1-introduce/1.1-cluster/",
	"title": "Cluster Architechture",
	"tags": [],
	"description": "",
	"content": "The architectural concepts behind Kubernetes A Kubernetes cluster consists of a control plane and one or more worker nodes. Here\u0026rsquo;s a brief overview of the main components:\n1. Control Plane Components: Manage the overall state of the cluster:\nkube-apiserver: The core component server that exposes the Kubernetes HTTP API\netcd: Consistent and highly-available key value store for all API server data\nkube-scheduler: Looks for Pods not yet bound to a node, and assigns each Pod to a suitable node.\nkube-controller-manager: Runs controllers to implement Kubernetes API behavior.\ncloud-controller-manager (optional): Integrates with underlying cloud provider(s).\n2. Worker Node Components: Run on every node, maintaining running pods and providing the Kubernetes runtime environment:\nkubelet: Ensures that Pods are running, including their containers.\nkube-proxy (optional): Maintains network rules on nodes to implement Services.\nContainer runtime: Software responsible for running containers. Read Container Runtimes to learn more.\nPods: A Pod is a smallest unit in Kubernetes, which contains at least one container. Containers in the same pod share their storage and network resources.\nServices: Services route requests to pods. Each service is associated with a group of pods and provide unified access to them\nVolumes: Volume is how container store, access and share data on disks.\nNamespace: Namespaces allow you to divide a cluster into smaller part with different resources and configurations\nIngress Controller: The Ingress Controller manages redirections of HTTP/HTTPS requests to Services in clusters based on rules configurations.\nStorage Classes: Storage classes define different types of storage that can be requested by persistent volume\nThe coordination of the components above forms an agile and powerful environment on a Kubernetes cluster for application deloyment.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-prepare-environment/2.1.1-cloud9/",
	"title": "Deploy Cloud9 Lab Environment (Deprecated - Not recommended)",
	"tags": [],
	"description": "",
	"content": "Attention: Cloud 9 has been deprecated and may not be supported for new accounts. We recommend moving to our suggested alternative solutions.\nHow to set up the environment to run the labs in your own AWS account. The first step is to create an IDE with the provided CloudFormation templates. You have the choice between using AWS Cloud9 or a browser-accessible instance of VSCode that will run on an EC2 instance in your AWS account.\nThese instructions have been tested in the following AWS regions and are not guaranteed to work in others without modification:\nRegion Cloud9 Link VSCode Link (Preview) us-west2 Launch Launch eu-west-1 Launch Launch ap-southeast-1 Launch Launch Alternatively, you can open CloudShell in the mentioned region and run the following command:\n$ wget -q https://raw.githubusercontent.com/aws-samples/eks-workshop-v2/stable/lab/cfn/eks-workshop-ide-cfn.yaml -O eks-workshop-ide-cfn.yaml $ aws cloudformation deploy --stack-name eks-workshop-ide \\ --template-file ./eks-workshop-ide-cfn.yaml \\ --parameter-overrides RepositoryRef=stable \\ --capabilities CAPABILITY_NAMED_IAM Waiting for changeset to be created... Waiting for stack create/update to complete Successfully created/updated stack - eks-workshop-ide The stack may take a few minutes to complete. When the stack creation is done, you can run the followin command in CloudShell to get the IDE\u0026rsquo;s URL:\naws cloudformation describe-stacks --stack-name eks-workshop-ide \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`Cloud9Url`].OutputValue\u0026#39; --output text https://us-west-2.console.aws.amazon.com/cloud9/ide/7b05513358534d11afeb7119845c5461?region=us-west-2 Open the URL in your browser to use Cloud9.\nYou can now close CloudShell, as all the following commands will be done in the Cloud9 Terminal. The AWS CLI is also install and will retrieve authentication information attached to the IDE. You can run the following command to check it:\n$ aws sts get-caller-identity The next step is to create an EKS cluster for the workshop. This will be in part 2.2.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-prepare-environment/",
	"title": "Environment Preperation",
	"tags": [],
	"description": "",
	"content": "Attention: Cloud 9 has been deprecated and may not be supported for new accounts. We recommend moving to our following alternative solutions.\nHow to set up the environment to run the labs in your own AWS account. Cloud9 (Deprecated and Not Recommended) Deploy VSCode as a web app with EC2 and CloudFront Deploying VSCode on EC2 and access it from your local VSCode Completely deploying on your local setting (connection and responsiveness depends on your region) "
},
{
	"uri": "//localhost:1313/",
	"title": "Getting started with Amazon Elastic Kubernetes Service",
	"tags": [],
	"description": "",
	"content": "Getting started with Amazon Elastic Kubernetes Service Agenda This workshop is part of a series of EKS workshops. Here, we provide an overview and help you to get familiar with deploying, configuring, and operating your applications on Kubernetes. With this particular workshop, we aim to provide a better understanding of how to create an EKS cluster, to execute commands, and to deploy some simple workloads on the cluster.\nKubernetes Kubernetes is a flexible, scalable, and open-sourced platform for containerized applications and associated services orchestration, making it easier to configure and automate the application development process. Known as a large and rapidly growing ecosystem, Kubernetes provides extensive support for a variety of services and tools.\nThe name Kubernetes comes from the Greek word for helmsman/pilot. Kubernetes was released to the public in 2014 by Google, based on Google\u0026rsquo;s recent decade of real-world workload management experience, combined with ideas and best practices from the community.\nWhy do you need Kubernetes and what can it do? Containers are a convenient way for you to contribute and run applications. In a production environment, there needs to be an efficient mechanism for managing containers, ensuring no downtime. Kubernetes helps manage powerful distributed systems, automates scaling, provides declarative development patterns, and much more.\nKubernetes provides:\nService discovery and load balancing Storage orchestration Automatic rollouts and rollbacks Automatic packaging Self-healing Configuration management and security Amazon Elastic Kubernetes Service (EKS) Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS).\nFeatures of Amazon EKS The following are key features of Amazon EKS:\nSecure networking and authentication Amazon EKS integrates your Kubernetes workloads with AWS networking and security services. It also integrates with AWS Identity and Access Management (IAM) to provide authentication for your Kubernetes clusters.\nEasy cluster scaling Amazon EKS enables you to scale your Kubernetes clusters up and down easily based on the demand of your workloads. Amazon EKS supports horizontal Pod autoscaling based on CPU or custom metrics, and cluster autoscaling based on the demand of the entire workload.\nManaged Kubernetes experience You can make changes to your Kubernetes clusters using eksctl, AWS Management Console, AWS Command Line Interface (AWS CLI), the API, kubectl, and Terraform.\nHigh availability Amazon EKS provides high availability for your control plane across multiple Availability Zones.\nIntegration with AWS services Amazon EKS integrates with other AWS services, providing a comprehensive platform for deploying and managing your containerized applications. You can also more easily troubleshoot your Kubernetes workloads with various observability tools.\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Kubernetes Kubernetes overview Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the \u0026ldquo;K\u0026rdquo; and the \u0026ldquo;s\u0026rdquo;. Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google\u0026rsquo;s experience running production workloads at scale with best-of-breed ideas and practices from the community.\nHistorical context for Kubernetes Let\u0026rsquo;s take a look at why Kubernetes is so useful by going back in time.\nTraditional deployment era: Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.\nVirtualized deployment era: As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server\u0026rsquo;s CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application.\nVirtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as a cluster of disposable virtual machines.\nEach VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.\nContainer deployment era: Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.\nContainers have become popular because they provide extra benefits, such as:\nAgile application creation and deployment: increased ease and efficiency of container image creation compared to VM image use.\nContinuous development, integration, and deployment: provides for reliable and frequent container image build and deployment with quick and efficient rollbacks (due to image immutability).\nDev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.\nObservability: not only surfaces OS-level information and metrics, but also application health and other signals.\nEnvironmental consistency across development, testing, and production: runs the same on a laptop as it does in the cloud.\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on major public clouds, and anywhere else.\nApplication-centric management: raises the level of abstraction from running an OS on virtual hardware to running an application on an OS using logical resources.\nLoosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can be deployed and managed dynamically – not a monolithic stack running on one big single-purpose machine.\nResource isolation: predictable application performance.\nResource utilization: high efficiency and density.\nWhy need Kubernetes ? What can it do ? Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn\u0026rsquo;t it be easier if this behavior was handled by a system?\nThat\u0026rsquo;s how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system.\nKubernetes provides you with:\nService discovery and load balancing: Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.\nAutomated rollouts and rollbacks: You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.\nAutomatic bin packing: You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.\nSelf-healing: Kubernetes restarts containers that fail, replaces containers, kills containers that don\u0026rsquo;t respond to your user-defined health check, and doesn\u0026rsquo;t advertise them to clients until they are ready to serve.\nSecret and configuration management: Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.\nBatch execution: In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.\nHorizontal scaling: Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.\nIPv4/IPv6: dual-stack Allocation of IPv4 and IPv6 addresses to Pods and Services\nDesigned for extensibility: Add features to your Kubernetes cluster without changing upstream source code.\nKubernetes is not Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, and lets users integrate their logging, monitoring, and alerting solutions. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.\nKubernetes:\nDoes not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.\nDoes not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and preferences as well as technical requirements.\nDoes not provide application-level services, such as middleware (for example, message buses), data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor cluster storage systems (for example, Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker.\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms to collect and export metrics.\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It provides a declarative API that may be targeted by arbitrary forms of declarative specifications.\nDoes not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems.\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn\u0026rsquo;t matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.\nAmazon Elastic Kubernetes Service (EKS) Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS).\nFeatures of Amazon EKS The following are key features of Amazon EKS:\nSecure networking and authentication Amazon EKS integrates your Kubernetes workloads with AWS networking and security services. It also integrates with AWS Identity and Access Management (IAM) to provide authentication for your Kubernetes clusters.\nEasy cluster scaling Amazon EKS enables you to scale your Kubernetes clusters up and down easily based on the demand of your workloads. Amazon EKS supports horizontal Pod autoscaling based on CPU or custom metrics, and cluster autoscaling based on the demand of the entire workload.\nManaged Kubernetes experience You can make changes to your Kubernetes clusters using eksctl, AWS Management Console, AWS Command Line Interface (AWS CLI), the API, kubectl, and Terraform.\nHigh availability Amazon EKS provides high availability for your control plane across multiple Availability Zones.\nIntegration with AWS services Amazon EKS integrates with other AWS services, providing a comprehensive platform for deploying and managing your containerized applications. You can also more easily troubleshoot your Kubernetes workloads with various observability tools.\n"
},
{
	"uri": "//localhost:1313/1-introduce/1.1-cluster/1.1.1-node/",
	"title": "Nodes",
	"tags": [],
	"description": "",
	"content": "Kubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.\nTypically you have several nodes in a cluster; in a learning or resource-limited environment, you might have only one node.\nThe components on a node include the kubelet, a container runtime, and the kube-proxy.\nManagement There are two main ways to have Nodes added to the API server:\nThe kubelet on a node self-registers to the control plane You (or another human user) manually add a Node object After you create a Node object, or the kubelet on a node self-registers, the control plane checks whether the new Node object is valid. For example, if you try to create a Node from the following JSON manifest:\n{ \u0026#34;kind\u0026#34;: \u0026#34;Node\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;10.240.79.157\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-first-k8s-node\u0026#34; } } } Kubernetes creates a Node object internally (the representation). Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node. If the node is healthy (i.e. all necessary services are running), then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity until it becomes healthy.\nNode name uniqueness The name identifies a Node. Two Nodes cannot have the same name at the same time. Kubernetes also assumes that a resource with the same name is the same object. In case of a Node, it is implicitly assumed that an instance using the same name will have the same state (e.g. network settings, root disk contents) and attributes like node labels. This may lead to inconsistencies if an instance was modified without changing its name. If the Node needs to be replaced or updated significantly, the existing Node object needs to be removed from API server first and re-added after the update.\nNode status A Node\u0026rsquo;s status contains the following information:\nAddresses\nConditions\nCapacity and Allocatable\nInfo\nYou can use kubectl to view a Node\u0026rsquo;s status and other details:\nkubectl describe node \u0026lt;insert-node-name-here\u0026gt; Resource capacity tracking Node objects track information about the Node\u0026rsquo;s resource capacity: for example, the amount of memory available and the number of CPUs. Nodes that self register report their capacity during registration. If you manually add a Node, then you need to set the node\u0026rsquo;s capacity information when you add it.\nThe Kubernetes scheduler ensures that there are enough resources for all the Pods on a Node. The scheduler checks that the sum of the requests of containers on the node is no greater than the node\u0026rsquo;s capacity. That sum of requests includes all containers managed by the kubelet, but excludes any containers started directly by the container runtime, and also excludes any processes running outside of the kubelet\u0026rsquo;s control.\n"
},
{
	"uri": "//localhost:1313/3-getting-started/1-sample-application/",
	"title": "Sample Aplication",
	"tags": [],
	"description": "",
	"content": "Most of the labs in this workshop use a common sample application to provide actual container components that we can work on during the exercises. The sample application models a simple web store application, where customers can browse a catalog, add items to their cart and complete an order through the checkout process.\nThe application has several components and dependencies:\nComponent Description UI Provides the front end user interface and aggregates API calls to the various other services. Catalog API for product listings and details Cart API for customer shopping carts Checkout API to orchestrate the checkout process Orders API to receive and process customer orders Static assets Serves static assets like images related to the product catalog Initially we\u0026rsquo;ll deploy the application in a manner that is self-contained in the Amazon EKS cluster, without using any AWS services like load balancers or a managed database. Over the course of the labs we\u0026rsquo;ll leverage different features of EKS to take advantage of broader AWS services and features for our retail store.\nYou can find the full source code for the sample application on GitHub.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-cluster-creation/2.2.1-eksctl/",
	"title": "Using eksctl",
	"tags": [],
	"description": "",
	"content": "How to build a cluster for the lab exercises using the eksctl tool. This is the easiest way to get started, and is recommended for most learners.\nThe eksctl utility has been pre-installed in your Amazon Cloud9 Environment, so we can immediately create the cluster. This is the configuration that will be used to build the cluster:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig availabilityZones: - ${AWS_REGION}a - ${AWS_REGION}b - ${AWS_REGION}c metadata: name: ${EKS_CLUSTER_NAME} region: ${AWS_REGION} version: \u0026#34;1.30\u0026#34; tags: karpenter.sh/discovery: ${EKS_CLUSTER_NAME} created-by: eks-workshop-v2 env: ${EKS_CLUSTER_NAME} iam: withOIDC: true vpc: cidr: 10.42.0.0/16 clusterEndpoints: privateAccess: true publicAccess: true addons: - name: vpc-cni version: 1.16.0 configurationValues: \u0026#39;{\u0026#34;env\u0026#34;:{\u0026#34;ENABLE_PREFIX_DELEGATION\u0026#34;:\u0026#34;true\u0026#34;, \u0026#34;ENABLE_POD_ENI\u0026#34;:\u0026#34;true\u0026#34;, \u0026#34;POD_SECURITY_GROUP_ENFORCING_MODE\u0026#34;:\u0026#34;standard\u0026#34;},\u0026#34;enableNetworkPolicy\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;nodeAgent\u0026#34;: {\u0026#34;enablePolicyEventLogs\u0026#34;: \u0026#34;true\u0026#34;}}\u0026#39; resolveConflicts: overwrite managedNodeGroups: - name: default desiredCapacity: 3 minSize: 3 maxSize: 6 instanceType: m5.large privateNetworking: true releaseVersion: \u0026#34;1.30.0-20240625\u0026#34; updateConfig: maxUnavailablePercentage: 50 labels: workshop-default: \u0026#34;yes\u0026#34; Based on this configuration eksctl will:\nCreate a VPC across three availability zones Create an EKS cluster Create an IAM OIDC provider Add a managed node group named default Configure the VPC CNI to use prefix delegation Apply the configuration file like so:\n$ export EKS_CLUSTER_NAME=eks-workshop $ curl -fsSL https://raw.githubusercontent.com/VAR::MANIFESTS_OWNER/VAR::MANIFESTS_REPOSITORY/VAR::MANIFESTS_REF/cluster/eksctl/cluster.yaml | \\ envsubst | eksctl create cluster -f - This process will take around 20 minutes.\nNext Steps Now that the cluster is ready, head to the Navigating the labs section or skip ahead to any module in the workshop with the top navigation bar. Once you\u0026rsquo;re completed with the workshop, follow the steps below to clean-up your environment.\nCleaning Up (steps once you are done with the Workshop) The following demonstrates how you will later clean up resources once you are done using the EKS cluster you created in previous steps to complete the modules.\nBefore deleting the Cloud9/VSCode IDE environment we need to clean up the cluster that we set up in previous steps.\nFirst, use delete-environment to ensure that the sample application and any left-over lab infrastructure is removed:\n$ delete-environment Next delete the cluster with eksctl:\n$ eksctl delete cluster $EKS_CLUSTER_NAME --wait "
},
{
	"uri": "//localhost:1313/1-introduce/1.2-containerruntimes/",
	"title": "Container Runtimes",
	"tags": [],
	"description": "",
	"content": "This section will provide you with an overview of container runtimes.\nWhat is a container runtime ? A container runtime is the foundational software that allows containers to operate within a host system. Container runtime is responsible for everything from pulling container images from a container registry and managing their life cycle to running the containers on your system.\nFunctionality and responsibilities Understanding core functionalities and responsibilities is vital to appreciating how container runtimes facilitate the seamless execution and management of containers. Generally, a container runtime do the following jobs:\nExecution of containers Container runtimes primarily execute containers through a multi-step process. As a first step, this process begins by creating containers and initializing their environment based on a container image that contains the application and its dependencies. Following creation, the runtime runs the containers, starts the application, and ensures its proper function. Additionally, the runtime manages containers’ life cycles, which involves monitoring their health, restarting them if they fail, and cleaning up resources once the containers are no longer in use.\nInteraction with the host operating system Container runtimes interact closely with the host operating system. They leverage various features of the OS, like namespaces and cgroups, to isolate and manage resources for each container. This isolation guarantees that processes inside a container are unable to disrupt the host or other containers, preserving a secure and stable environment.\nResource allocation and management Container runtimes are an essential part of resource management because they allocate and regulate CPU, memory, and I/O for each container to prevent resource monopolization, especially in multi-tenant environments. The way container runtimes smoothly handle the running, life cycle, and interaction of containers with the host OS is key to why containerization is such a big part of today\u0026rsquo;s software development landscape.\nLow-Level and High-Level Container Runtimes Talking of container runtimes, a list of examples might come to mind: runc, lxc, lmctfy, Docker (containerd), rkt, cri-o. Each of these is built for different situations and implements different features. Some, like containerd and cri-o, actually use runc to run the container but implement image management and APIs on top. You can think of these features – which include image transport, image management, image unpacking, and APIs – as high-level features as compared to runc’s low-level implementation.\nWith that in mind you can see that the container runtime space is fairly complicated. Each runtime covers different parts of this low-level to high-level spectrum. Here is a very subjective diagram:\nTherefore, for practical purposes, actual container runtimes that focus on just running containers are usually referred to as “low-level container runtimes”. Runtimes that support more high-level features, like image management and gRPC/Web APIs, are usually referred to as “high-level container tools”, “high-level container runtimes” or usually just “container runtimes”. This section will refer to them as “high-level container runtimes”. It’s important to note that low-level runtimes and high-level runtimes are fundamentally different things that solve different problems.\nTypically, developers who want to run apps in containers will need more than just the features that low-level runtimes provide. They need APIs and features around image formats, image management, and sharing images. These features are provided by high-level runtimes. Low-level runtimes just don’t provide enough features for this everyday use. For that reason those that will actually use low-level runtimes would only be developers who implement higher level runtimes, and tools for containers.\nDevelopers who implement low-level runtimes will say that higher level runtimes like containerd and cri-o are not actually container runtimes, as from their perspective they outsource the implementation of running a container to runc. However, from the user’s perspective, they are a singular component that provides the ability to run containers.\nKubernetes container runtimes Kubernetes runtimes are high-level container runtimes that support the Container Runtime Interface (CRI). CRI was introduced in Kubernetes 1.5 and acts as a bridge between the kubelet and the container runtime. High-level container runtimes that want to integrate with Kubernetes are expected to implement CRI. The runtime is expected to handle the management of images and to support Kubernetes pods, as well as manage the individual containers, and therefore, is considered a high-level runtime by the categorisation above.\nIn order to understand more about CRI it’s worth taking once more look at the overall Kubernetes architecture. The kubelet is an agent that sits on each worker node in the Kubernetes cluster. The kubelet is responsible for managing the container workloads for its node. When it comes to actually run the workload, the kubelet uses CRI to communicate with the container runtime running on that same node. In this way CRI is simply an abstraction layer or API that allows you to switch out container runtime implementations instead of having them built into the kubelet.\nBelow are some CRI runtimes that can be used with Kubernetes:\nDocker Docker is one of the first open source container runtimes. It was developed by the platform-as-a-service company dotCloud, and was used to run their users’ web applications in containers.\nDocker is a container runtime that incorporates building, packaging, sharing, and running containers. Docker has a client/server architecture and was originally built as a monolithic daemon, dockerd, and the docker client application. The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API. The command line client could be run to send commands and to get information from the daemon.\nDocker originally implemented both high-level and low-level runtime features, but those pieces have since been broken out into separate projects as runc and containerd. Docker now consists of the dockerd daemon, and the docker-containerd daemon along with docker-runc. docker-containerd and docker-runc are just Docker packaged versions of vanilla containerd and runc.\ndockerd provides features such as building images, and dockerd uses docker-containerd to provide features such as image management and running containers. For instance, Docker’s build step is actually just some logic that interprets a Dockerfile, runs the necessary commands in a container using containerd, and saves the resulting container file system as an image.\ncontainerd containerd is a high-level runtime that was split off from Docker. Like runc, which was broken off as the low-level runtime piece, containerd was broken off as the high-level runtime piece of Docker. containerd implements downloading images, managing them, and running containers from images. When it needs to run a container it unpacks the image into an OCI runtime bundle and shells out to runc to run it.\ncontainerd also provides an API and client application that can be used to interact with it. The containerd command line client is ctr.\nctr can be used to tell containerd to pull a container image:\nsudo ctr images pull docker.io/library/redis:latest List the images you have:\nsudo ctr images list Run a container based on an image:\nsudo ctr container create docker.io/library/redis:latest redis List the running containers:\nsudo ctr container list Stop the container:\nsudo ctr container delete redis These commands are similar to how a user interacts with Docker. However, in contrast with Docker, containerd is focused solely on running containers, so it does not provide a mechanism for building containers. Docker was focused on end-user and developer use cases, whereas containerd is focused on operational use cases, such as running containers on servers. Tasks such as building container images are left to other tools.\ncri-o cri-o is a lightweight CRI runtime made as a Kubernetes specific high-level runtime. It supports the management of OCI compatible images and pulls from any OCI compatible image registry. It supports runc and Clear Containers as low-level runtimes. It supports other OCI compatible low-level runtimes in theory, but relies on compatibility with the runc OCI command line interface, so in practice it isn’t as flexible as containerd’s shim API.\ncri-o’s endpoint is at /var/run/crio/crio.sock by default so you can configure crictl like so. cat \u0026lt;\u0026lt;EOF | sudo tee /etc/crictl.yaml runtime-endpoint: unix:///var/run/crio/crio.sock EOF Interacting with CRI We can interact with a CRI runtime directly using the crictl tool. crictl lets us send gRPC messages to a CRI runtime directly from the command line. We can use this to debug and test out CRI implementations without starting up a full-blown kubelet or Kubernetes cluster. You can get it by downloading a crictl binary from the cri-tools releases page on GitHub.\nYou can configure crictl by creating a configuration file under /etc/crictl.yaml. Here you should specify the runtime’s gRPC endpoint as either a Unix socket file (unix:///path/to/file) or a TCP endpoint (tcp://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;). We will use containerd for this example:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/crictl.yaml runtime-endpoint: unix:///run/`containerd`/`containerd`.sock EOF Or you can specify the runtime endpoint on each command line execution:\ncrictl --runtime-endpoint unix:///run/`containerd`/`containerd`.sock … Let’s run a pod with a single container with crictl. First you would tell the runtime to pull the nginx image you need since you can’t start a container without the image stored locally.\nsudo crictl pull nginx Next create a Pod creation request. You do this as a JSON file.\ncat \u0026lt;\u0026lt;EOF | tee sandbox.json { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx-sandbox\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;attempt\u0026#34;: 1, \u0026#34;uid\u0026#34;: \u0026#34;hdishd83djaidwnduwk28bcsb\u0026#34; }, \u0026#34;linux\u0026#34;: { }, \u0026#34;log_directory\u0026#34;: \u0026#34;/tmp\u0026#34; } EOF And then create the pod sandbox. We will store the ID of the sandbox as SANDBOX_ID.\nSANDBOX_ID=$(sudo crictl runp --runtime runsc sandbox.json) Next we will create a container creation request in a JSON file. cat \u0026lt;\u0026lt;EOF | tee container.json { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34; }, \u0026#34;image\u0026#34;:{ \u0026#34;image\u0026#34;: \u0026#34;nginx\u0026#34; }, \u0026#34;log_path\u0026#34;:\u0026#34;nginx.0.log\u0026#34;, \u0026#34;linux\u0026#34;: { } } EOF We can then create and start the container inside the Pod we created earlier.\nCONTAINER_ID=$(sudo crictl create ${SANDBOX_ID} container.json sandbox.json) sudo crictl start ${CONTAINER_ID} You can inspect the running pod\nsudo crictl inspectp ${SANDBOX_ID} … and the running container:\nsudo crictl inspect ${CONTAINER_ID} Clean up by stopping and deleting the container:\nsudo crictl stop ${CONTAINER_ID} sudo crictl rm ${CONTAINER_ID} And then stop and delete the Pod:\nsudo crictl stopp ${SANDBOX_ID} sudo crictl rmp ${SANDBOX_ID} References https://www.wiz.io/academy/container-runtimes https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-cluster-creation/",
	"title": "Creating your Kubernetes Cluster",
	"tags": [],
	"description": "",
	"content": "In this step, we create an EKS cluster to perform the lab exercises in. Please follow one of the guides below to provision a cluster that meets the requirements for these labs:\n(Recommended) eksctl Terraform CDK (TBA) "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-prepare-environment/2.1.2-cloud-ide/",
	"title": "Deploy VSCode as a web app with EC2 and CloudFront",
	"tags": [],
	"description": "",
	"content": "\rProvisioning this workshop environment in your AWS account will create resources and there will be cost associated with them. The cleanup section provides a guide to remove them, preventing further charges.\nThis section outlines how to set up the environment to run the labs in your own AWS account.\nThe first step is to create an IDE with the provided CloudFormation templates. You have the choice between using AWS Cloud9 or a browser-accessible instance of VSCode that will run on an EC2 instance in your AWS account.\nUse the AWS CloudFormation quick-create links below to launch the desired template in the appropriate AWS region.\nRegion Link (Preview) us-west2 Launch eu-west-1 Launch ap-southeast-1 Launch These instructions have been tested in the AWS regions listed above and are not guaranteed to work in others without modification.\nThe nature of the workshop material means that the IDE EC2 instance requires broad IAM permissions in your account, for example creating IAM roles. Before continuing please review the IAM permissions that will be provided to the IDE instance in the CloudFormation template. We are continuously working to optimize the IAM permissions. Please raise a GitHub issue with any suggestions for improvement.\nNow select that tab that corresponds to the IDE that you have installed.\nScroll to the bottom of the screen and acknowledge the IAM notice:\nThen click the Create stack button:\nThe CloudFormation stack will take roughly 5 minutes to deploy, and once completed you can retrieve information required to continue from the Outputs tab:\nThe IdeUrl output contains the URL to enter in your browser to access the IDE. The IdePasswordSecret contains a link to an AWS Secrets Manger secret that contains a generated password for the IDE.\nTo retrieve the password open that URL and click the Retrieve button:\nThe password will then be available for you to copy:\nOpen the IDE URL provided and you will be prompted for the password:\nAfter submitting your password you will be presented with the initial VSCode screen:\nIn case CloudFormation failed to create a CloudFront Distribution resource due to your account not being verified, you can request limit increase for CloudFront Distribution here. At Service, choose \u0026ldquo;CloudFront Distribution\u0026rdquo;. At Request 1, choose Web Distributions per Account for Quota set a higher value than your previous value (if you\u0026rsquo;re using a new account, just enter any positive number) At Case description, copy the detailed error return from CloudStack, at the CloudFront Distribution creation step, and paste it, then enter a brief description of your problem. After that, choose a contact method, then click Submit at the bottom. Now you wait for up to a few days for the Support center to work on your case.\n"
},
{
	"uri": "//localhost:1313/3-getting-started/2-lab-components/",
	"title": "Lab Components",
	"tags": [],
	"description": "",
	"content": "Before a workload can be deployed to a Kubernetes distribution like EKS it first must be packaged as a container image and published to a container registry. Basic container topics like this are not covered as part of this workshop, and the sample application has container images already available in Amazon Elastic Container Registry for the labs we\u0026rsquo;ll complete today.\nThe table below provides links to the ECR Public repository for each component, as well as the Dockerfile that was used to build each component.\nComponent ECR Public repository Dockerfile UI Repository Dockerfile Catalog Repository Dockerfile Shopping cart Repository Dockerfile Checkout Repository Dockerfile Orders Repository Dockerfile Assets Repository Dockerfile "
},
{
	"uri": "//localhost:1313/1-introduce/1.1-cluster/1.1.2-communitcation/",
	"title": "Nodes - Control Plane communication",
	"tags": [],
	"description": "",
	"content": "This document catalogs the communication paths between the API server and the Kubernetes cluster. The intent is to allow users to customize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud provider).\nNode to Control Plane Kubernetes has a \u0026ldquo;hub-and-spoke\u0026rdquo; API pattern. All API usage from nodes (or the pods they run) terminates at the API server. None of the other control plane components are designed to expose remote services. The API server is configured to listen for remote connections on a secure HTTPS port (typically 443) with one or more forms of client authentication enabled. One or more forms of authorization should be enabled, especially if anonymous requests or service account tokens are allowed.\nNodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the API server along with valid client credentials. A good approach is that the client credentials provided to the kubelet are in the form of a client certificate. See kubelet TLS bootstrapping for automated provisioning of kubelet client certificates.\nPods that wish to connect to the API server can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in default namespace) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the API server.\nThe control plane components also communicate with the API server over the secure port.\nAs a result, the default operating mode for connections from the nodes and pod running on the nodes to the control plane is secured by default and can run over untrusted and/or public networks.\nControl plane to node There are two primary communication paths from the control plane (the API server) to the nodes. The first is from the API server to the kubelet process which runs on each node in the cluster. The second is from the API server to any node, pod, or service through the API server\u0026rsquo;s proxy functionality.\nAPI server to kubelet The connections from the API server to the kubelet are used for:\nFetching logs for pods.\nAttaching (usually through kubectl) to running pods.\nProviding the kubelet\u0026rsquo;s port-forwarding functionality.\nThese connections terminate at the kubelet\u0026rsquo;s HTTPS endpoint. By default, the API server does not verify the kubelet\u0026rsquo;s serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks.\nTo verify this connection, use the \u0026ndash;kubelet-certificate-authority flag to provide the API server with a root certificate bundle to use to verify the kubelet\u0026rsquo;s serving certificate.\nIf that is not possible, use SSH tunneling between the API server and kubelet if required to avoid connecting over an untrusted or public network.\nFinally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.\nAPI server to nodes, pods, and services The connections from the API server to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So while the connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run over untrusted or public networks.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "Before getting into the labs of this workshop, the following three steps must be done:\nPrepare your Environment:\nCreate and prepare your system to connect to AWS.\nPrepare your IDE/code editor and source code tree.\nCreate your EKS Cluster:\nCreate an EKS Cluster where your workload is deployed.\nEither with CloudFormation or Terraform.\nGet to know the Lab Structure\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-cluster-creation/2.2.2-terraform/",
	"title": "Using Terraform",
	"tags": [],
	"description": "",
	"content": "How to build a cluster for the lab exercises using the Hashicorp Terraform. This is intended to be for learners that are used to working with Terraform infrastructure-as-code.\nThe terraform CLI has been pre-installed in your IDE so we can immediately create the cluster. Let\u0026rsquo;s take a look at the main Terraform configuration files that will be used to build the cluster and its supporting infrastructure.\nUnderstanding Terraform config files The providers.tf file configures the Terraform providers that will be needed to build the infrastructure. In our case, we use the aws, kubernetes and helm providers:\nprovider \u0026#34;aws\u0026#34; {\rdefault_tags {\rtags = local.tags\r}\r}\rterraform {\rrequired_providers {\raws = {\rsource = \u0026#34;hashicorp/aws\u0026#34;\rversion = \u0026#34;\u0026gt;= 4.67.0\u0026#34;\r}\r}\rrequired_version = \u0026#34;\u0026gt;= 1.4.2\u0026#34;\r} The main.tf file sets up some Terraform data sources so we can retrieve the current AWS account and region being used, as well as some default tags:\nlocals {\rtags = {\rcreated-by = \u0026#34;eks-workshop-v2\u0026#34;\renv = var.cluster_name\r}\r} The vpc.tf configuration will make sure our VPC infrastructure is created:\nlocals {\rprivate_subnets = [for k, v in local.azs : cidrsubnet(var.vpc_cidr, 3, k + 3)]\rpublic_subnets = [for k, v in local.azs : cidrsubnet(var.vpc_cidr, 3, k)]\razs = slice(data.aws_availability_zones.available.names, 0, 3)\r}\rdata \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; {\rstate = \u0026#34;available\u0026#34;\r}\rmodule \u0026#34;vpc\u0026#34; {\rsource = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34;\rversion = \u0026#34;~\u0026gt; 5.1\u0026#34;\rname = var.cluster_name\rcidr = var.vpc_cidr\razs = local.azs\rpublic_subnets = local.public_subnets\rprivate_subnets = local.private_subnets\rpublic_subnet_suffix = \u0026#34;SubnetPublic\u0026#34;\rprivate_subnet_suffix = \u0026#34;SubnetPrivate\u0026#34;\renable_nat_gateway = true\rcreate_igw = true\renable_dns_hostnames = true\rsingle_nat_gateway = true\r# Manage so we can name\rmanage_default_network_acl = true\rdefault_network_acl_tags = { Name = \u0026#34;${var.cluster_name}-default\u0026#34; }\rmanage_default_route_table = true\rdefault_route_table_tags = { Name = \u0026#34;${var.cluster_name}-default\u0026#34; }\rmanage_default_security_group = true\rdefault_security_group_tags = { Name = \u0026#34;${var.cluster_name}-default\u0026#34; }\rpublic_subnet_tags = merge(local.tags, {\r\u0026#34;kubernetes.io/role/elb\u0026#34; = \u0026#34;1\u0026#34;\r})\rprivate_subnet_tags = merge(local.tags, {\r\u0026#34;karpenter.sh/discovery\u0026#34; = var.cluster_name\r\u0026#34;kubernetes.io/role/internal-elb\u0026#34; = \u0026#34;1\u0026#34;\r})\rtags = local.tags\r} Finally, the eks.tf file specifies our EKS cluster configuration, including a Managed Node Group:\nmodule \u0026#34;eks\u0026#34; {\rsource = \u0026#34;terraform-aws-modules/eks/aws\u0026#34;\rversion = \u0026#34;~\u0026gt; 20.0\u0026#34;\rcluster_name = var.cluster_name\rcluster_version = var.cluster_version\rcluster_endpoint_public_access = true\renable_cluster_creator_admin_permissions = true\rcluster_addons = {\rvpc-cni = {\rbefore_compute = true\rmost_recent = true\rconfiguration_values = jsonencode({\renv = {\rENABLE_POD_ENI = \u0026#34;true\u0026#34;\rENABLE_PREFIX_DELEGATION = \u0026#34;true\u0026#34;\rPOD_SECURITY_GROUP_ENFORCING_MODE = \u0026#34;standard\u0026#34;\r}\rnodeAgent = {\renablePolicyEventLogs = \u0026#34;true\u0026#34;\r}\renableNetworkPolicy = \u0026#34;true\u0026#34;\r})\r}\r}\rvpc_id = module.vpc.vpc_id\rsubnet_ids = module.vpc.private_subnets\rcreate_cluster_security_group = false\rcreate_node_security_group = false\reks_managed_node_groups = {\rdefault = {\rinstance_types = [\u0026#34;m5.large\u0026#34;]\rforce_update_version = true\rrelease_version = var.ami_release_version\ruse_name_prefix = false\riam_role_name = \u0026#34;${var.cluster_name}-ng-default\u0026#34;\riam_role_use_name_prefix = false\rmin_size = 3\rmax_size = 6\rdesired_size = 3\rupdate_config = {\rmax_unavailable_percentage = 50\r}\rlabels = {\rworkshop-default = \u0026#34;yes\u0026#34;\r}\r}\r}\rtags = merge(local.tags, {\r\u0026#34;karpenter.sh/discovery\u0026#34; = var.cluster_name\r})\r} Creating the workshop environment with Terraform For the given configuration, terraform will create the Workshop environment with the following:\nCreate a VPC across three availability zones Create an EKS cluster Create an IAM OIDC provider Add a managed node group named default Configure the VPC CNI to use prefix delegation Download the Terraform files:\n$ mkdir -p ~/environment/terraform; cd ~/environment/terraform $ curl --remote-name-all https://raw.githubusercontent.com/VAR::MANIFESTS_OWNER/VAR::MANIFESTS_REPOSITORY/VAR::MANIFESTS_REF/cluster/terraform/{main.tf,variables.tf,providers.tf,vpc.tf,eks.tf} Run the following Terraform commands to deploy your workshop environment.\n$ export EKS_CLUSTER_NAME=eks-workshop $ terraform init $ terraform apply -var=\u0026#34;cluster_name=$EKS_CLUSTER_NAME\u0026#34; -auto-approve This generally takes 20-25 minutes to complete.\nNext Steps Now that the cluster is ready, head to the Navigating the labs section or skip ahead to any module in the workshop with the top navigation bar. Once you\u0026rsquo;re completed with the workshop, follow the steps below to clean-up your environment.\nCleaning Up The following demonstrates how you will later clean up resources once you have completed your desired lab exercises. These steps will delete all provisioned infrastructure.\nBefore deleting the Cloud9/VSCode IDE environment we need to clean up the cluster that we set up above.\nFirst use delete-environment to ensure that the sample application and any left-over lab infrastructure is removed:\n$ delete-environment Next delete the cluster with terraform:\n$ cd ~/environment/terraform $ terraform destroy -var=\u0026#34;cluster_name=$EKS_CLUSTER_NAME\u0026#34; -auto-approve "
},
{
	"uri": "//localhost:1313/1-introduce/1.1-cluster/1.1.3-controllers/",
	"title": "Controllers",
	"tags": [],
	"description": "",
	"content": "In robotics and automation, a control loop is a non-terminating loop that regulates the state of a system.\nHere is one example of a control loop: a thermostat in a room.\nWhen you set the temperature, that\u0026rsquo;s telling the thermostat about your desired state. The actual room temperature is the current state. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off. In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.\nController pattern A controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state. The controller(s) for that resource are responsible for making the current state come closer to that desired state.\nThe controller might carry the action out itself; more commonly, in Kubernetes, a controller will send messages to the API server that have useful side effects. You\u0026rsquo;ll see examples of this below.\nControl via API server The Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage state by interacting with the cluster API server.\nJob is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and then stop.\n(Once scheduled, Pod objects become part of the desired state for a kubelet).\nWhen the Job controller sees a new task it makes sure that, somewhere in your cluster, the kubelets on a set of Nodes are running the right number of Pods to get the work done. The Job controller does not run any Pods or containers itself. Instead, the Job controller tells the API server to create or remove Pods. Other components in the control plane act on the new information (there are new Pods to schedule and run), and eventually the work is done.\nAfter you create a new Job, the desired state is for that Job to be completed. The Job controller makes the current state for that Job be nearer to your desired state: creating Pods that do the work you wanted for that Job, so that the Job is closer to completion.\nControllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates that Job object to mark it Finished.\n(This is a bit like how some thermostats turn a light off to indicate that your room is now at the temperature you set).\nDirect control In contrast with Job, some controllers need to make changes to things outside of your cluster.\nFor example, if you use a control loop to make sure there are enough Nodes in your cluster, then that controller needs something outside the current cluster to set up new Nodes when needed.\nControllers that interact with external state find their desired state from the API server, then communicate directly with an external system to bring the current state closer in line.\n(There actually is a controller that horizontally scales the nodes in your cluster.)\nThe important point here is that the controller makes some changes to bring about your desired state, and then reports the current state back to your cluster\u0026rsquo;s API server. Other control loops can observe that reported data and take their own actions.\nIn the thermostat example, if the room is very cold then a different controller might also turn on a frost protection heater. With Kubernetes clusters, the control plane indirectly works with IP address management tools, storage services, cloud provider APIs, and other services by extending Kubernetes to implement that.\nDesired versus current state Kubernetes takes a cloud-native view of systems, and is able to handle constant change.\nYour cluster could be changing at any point as work happens and control loops automatically fix failures. This means that, potentially, your cluster never reaches a stable state.\nAs long as the controllers for your cluster are running and able to make useful changes, it doesn\u0026rsquo;t matter if the overall state is stable or not.\nDesign As a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect of cluster state. Most commonly, a particular control loop (controller) uses one kind of resource as its desired state, and has a different kind of resource that it manages to make that desired state happen. For example, a controller for Jobs tracks Job objects (to discover new work) and Pod objects (to run the Jobs, and then to see when the work is finished). In this case something else creates the Jobs, whereas the Job controller creates Pods.\nIt\u0026rsquo;s useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so Kubernetes is designed to allow for that.\nThere can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers make sure that they only pay attention to the resources linked to their controlling resource.\nFor example, you can have Deployments and Jobs; these both create Pods. The Job controller does not delete the Pods that your Deployment created, because there is information (labels) the controllers can use to tell those Pods apart.\nWays of running controllers Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager. These built-in controllers provide important core behaviors.\nThe Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself (\u0026ldquo;built-in\u0026rdquo; controllers). Kubernetes lets you run a resilient control plane, so that if any of the built-in controllers were to fail, another part of the control plane will take over the work.\nYou can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller yourself. You can run your own controller as a set of Pods, or externally to Kubernetes. What fits best will depend on what that particular controller does.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-prepare-environment/2.1.3-remote-vscode/",
	"title": "Deploying VSCode on EC2 and access it from your local VSCode",
	"tags": [],
	"description": "",
	"content": "Create your Key Pair If you already have a key pair, you can skip to the next step.\nFirst, you need to enter the EC2 Console. On the left navigator bar, choose Key Pairs. On the Key Pairs interface, choose Create Key Pair.\nOn Create Key Pair:\nAt Name, enter any name for your key pair. Next, choose a key type and the .pem format. Click Create key pair at the bottom to proceed to create your key pair. Save your private key in your local computer and use it later for the lab stack creation.\nEnsure that only you can access the .pem file.\nOn Linux, use the following command: chmod 400 \u0026lt;đường dẫn tới file pem\u0026gt; On Windows, ensure the file access permission is as follow: Create your CloudFormation Stack Dowload the template here.\nEnter the CloudFormation console.\nChoose Create Stack. Enter **Stack Name**s.\nAt SshKeyName, enter the newly created Key Pair (from the previous step).\nClick Next through all steps and wait for the stack to complete.\nConfigure SSH connection on your computer On the CloudFormation Stack UI, get to Output. Find the key IdeInstancePublicIP, the value is the address of your VSCode server instance.\nAdd the following lines C:\\Users\\\u0026lt;username\u0026gt;\\.ssh\\config (on Windows) hoặc ~/.ssh/config (on Linux):\nHost remote-connection\rHostName \u0026lt;EC2 public ip\u0026gt;\rUser ec2-user\rIdentityFile \u0026#34;\u0026lt;path-to-pem\u0026gt;\u0026#34; Connect your VSCode with your EC2 Instance Install the Remote-SSH addon for your local VSCode After installing you\u0026rsquo;ll see the icon on the lower left corner of the screen\nClick the icon to open the Connection command pallete. Choose “Connect to Host…”\nChoose “Add New SSH Host” Choose your config file\nAfter configuring, click the icon to open the pallette and choose “remote-connection”\nNext, choose Platform details “Linux”, then “Continue”\nChoose \u0026ldquo;File \u0026gt; Open\u0026rdquo; to open your environment folder\nChoose the folder /home/\u0026lt; username \u0026gt;/environment\n"
},
{
	"uri": "//localhost:1313/1-introduce/1.3-etcd/",
	"title": "ETCD",
	"tags": [],
	"description": "",
	"content": "What is a ETCD? ETCD is a distributed reliable key-value store that is simple, secure \u0026amp; Fast. What is a Key-Value Store Traditionally, databases have been in tabular format, you must have heard about SQL or Relational databases. They store data in rows and columns\nA Key-Value Store stores information in a Key and Value format.\nInstall ETCD It\u0026rsquo;s easy to install and get started with ETCD. Download the relevant binary for your operating system from github releases page (https://github.com/etcd-io/etcd/releases) For Example: To download ETCD v3.5.6, run the below curl command\n$ curl -LO https://github.com/etcd-io/etcd/releases/download/v3.5.6/etcd-v3.5.6-linux-amd64.tar.gz Extract it. $ tar xvzf etcd-v3.5.6-linux-amd64.tar.gz Run the ETCD Service $ ./etcd When you start ETCD it will by default listen on port 2379 The default client that comes with ETCD is the etcdctl client. You can use it to store and retrieve key-value pairs. Syntax: To Store a Key-Value pair\r$ ./etcdctl put key1 value1 Syntax: To retrieve the stored data\r$ ./etcdctl get key1 Syntax: To view more commands. Run etcdctl without any arguments\r$ ./etcdctl ETCD Datastore The ETCD Datastore stores information regarding the cluster such as Nodes, PODS, Configs, Secrets, Accounts, Roles, Bindings and Others. Every information you see when you run the kubectl get command is from the ETCD Server. Setup - Manual If you setup your cluster from scratch then you deploy ETCD by downloading ETCD Binaries yourself Installing Binaries and Configuring ETCD as a service in your master node yourself. $ wget -q --https-only \u0026#34;https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz\u0026#34; Setup - Kubeadm If you setup your cluster using kubeadm then kubeadm will deploy etcd server for you as a pod in kube-system namespace. $ kubectl get pods -n kube-system Explore ETCD To list all keys stored by kubernetes, run the below command $ kubectl exec etcd-master -n kube-system -- sh -c \u0026#34;ETCDCTL_API=3 etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt get / --prefix --keys-only\u0026#34; Kubernetes Stores data in a specific directory structure, the root directory is the registry and under that you have various kubernetes constructs such as minions, nodes, pods, replicasets, deployments, roles, secrets and Others. ETCD in HA Environment In a high availability environment, you will have multiple master nodes in your cluster that will have multiple ETCD Instances spread across the master nodes. Make sure etcd instances know each other by setting the right parameter in the etcd.service configuration. The --initial-cluster option where you need to specify the different instances of the etcd service. Reference Docs: https://kubernetes.io/docs/concepts/overview/components/ https://etcd.io/docs/ https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#external-etcd-nodes "
},
{
	"uri": "//localhost:1313/3-getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Welcome to the first hands-on lab in the EKS workshop. The goal of this exercise is to familiarize ourselves with the sample application we\u0026rsquo;ll use for many of the coming lab exercises and in doing so touch on some basic concepts related to deploying workloads to EKS. We\u0026rsquo;ll explore the architecture of the application and deploy out the components to our EKS cluster.\nLet\u0026rsquo;s deploy your first workload to the EKS cluster in your lab environment and explore!\nBefore we begin we need to run the following command to prepare our IDE environment and EKS cluster:\n$ prepare-environment introduction/getting-started What is this command doing? For this lab it is cloning the EKS Workshop Git repository in to the IDE environment so the Kubernetes manifest files we need are present on the file system.\nYou\u0026rsquo;ll notice in subsequent labs we\u0026rsquo;ll also run this command, where it will perform two important additional functions:\nReset the EKS cluster back to its initial state Install any additional components needed in to the cluster for the upcoming lab exercise "
},
{
	"uri": "//localhost:1313/3-getting-started/3-microservices-on-kubernetes/",
	"title": "Microservices on Kubernetes",
	"tags": [],
	"description": "",
	"content": "Now that we\u0026rsquo;re familiar with the overall architecture of the sample application, how will we initially deploy this in to EKS? Let\u0026rsquo;s explore some of the Kubernetes building blocks by looking at the catalog component:\nThere are a number of things to consider in this diagram:\nThe application that provides the catalog API runs as a Pod, which is the smallest deployable unit in Kubernetes. Application Pods will run the container images we outlined in the previous section. The Pods that run for the catalog component are created by a Deployment which may manage one or more \u0026ldquo;replicas\u0026rdquo; of the catalog Pod, allowing it to scale horizontally. A Service is an abstract way to expose an application running as a set of Pods, and this allows our catalog API to be called by other components inside the Kubernetes cluster. Each Service is given its own DNS entry. We\u0026rsquo;re starting this workshop with a MySQL database that runs inside our Kubernetes cluster as a StatefulSet, which is designed to manage stateful workloads. All of these Kubernetes constructs are grouped in their own dedicated catalog Namespace. Each of the application components has its own Namespace. Each of the components in the microservices architecture is conceptually similar to the catalog, using Deployments to manage application workload Pods and Services to route traffic to those Pods. If we expand out our view of the architecture we can consider how traffic is routed throughout the broader system:\nThe ui component receives HTTP requests from, for example, a users browser. It then makes HTTP requests to other API components in the architecture to fulfill that request and returns a response to the user. Each of the downstream components may have their own data stores or other infrastructure. The Namespaces are a logical grouping of the resources for each microservice and also act as a soft isolation boundary, which can be used to effectively implement controls using Kubernetes RBAC and Network Policies.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-structure/",
	"title": "Workshop Structure",
	"tags": [],
	"description": "",
	"content": "Let’s review how to navigate this web site and the content provided.\nStructure The content of this workshop is made up of:\nIndividual lab exercises Supporting content that explains concepts related to the labs You should start each lab from the first page. Starting in the middle of a lab will cause unpredictable behavior.\nOnce you have accessed the Cloud9 IDE, we recommend you use the + button and select New Terminal to open a new full screen terminal window.\nThis will open a new tab with a fresh terminal.\nYou may also close the small terminal at the bottom if you wish.\nTerminal commands Most of the interaction you will do in this workshop will be done with terminal commands, which you can either manually type or copy/paste to the Cloud9 IDE terminal. You will see this terminal commands displayed like this:\n$ echo \u0026#34;This is an example command\u0026#34; Hover your mouse over echo \u0026quot;This is an example command\u0026quot; and click to copy that command to your clipboard.\nYou will also come across commands with sample output like this:\n$ date Fri Aug 30 12:25:58 MDT 2024 Using the \u0026lsquo;click to copy\u0026rsquo; function will only copy the command and ignore the sample output.\nAnother pattern used in the content is presenting several commands in a single terminal:\n$ echo \u0026#34;This is an example command\u0026#34; This is an example command $ date Fri Aug 30 12:26:58 MDT 2024 In this case you can either copy each command individually or copy all of the commands using the clipboard icon in the top right of the terminal window. Give it a shot!\nResetting your EKS cluster In the event that you accidentally configure your cluster in a way that is not functioning you have been provided with a mechanism to reset your EKS cluster as best we can which can be run at any time. Simply run the command prepare-environment and wait until it completes. This may take several minutes depending on the state of your cluster when it is run.\n"
},
{
	"uri": "//localhost:1313/3-getting-started/4-deploying-our-first-component/",
	"title": "Deploy our first component",
	"tags": [],
	"description": "",
	"content": "The sample application is composed of a set of Kubernetes manifests organized in a way that can be easily applied with Kustomize. Kustomize is an open-source tool also provided as a native feature of the kubectl CLI. This workshop uses Kustomize to apply changes to Kubernetes manifests, making it easier to understand changes to manifest files without needing to manually edit YAML. As we work through the various modules of this workshop, we\u0026rsquo;ll incrementally apply overlays and patches with Kustomize.\nThe easiest way to browse the YAML manifests for the sample application and the modules in this workshop is using the file browser in the IDE:\nExpanding the eks-workshop and then base-application items will allow you to browse the manifests that make up the initial state of the sample application:\nThe structure consists of a directory for each application component that was outlined in the Sample application section.\nThe modules directory contains sets of manifests that we will apply to the cluster throughout the subsequent lab exercises:\nBefore we do anything lets inspect the current Namespaces in our EKS cluster:\n$ kubectl get namespaces NAME STATUS AGE default Active 1h kube-node-lease Active 1h kube-public Active 1h kube-system Active 1h All of the entries listed are Namespaces for system components that were pre-installed for us. We\u0026rsquo;ll ignore these by using Kubernetes labels to filter the Namespaces down to only those we\u0026rsquo;ve created:\n$ kubectl get namespaces -l app.kubernetes.io/created-by=eks-workshop No resources found The first thing we\u0026rsquo;ll do is deploy the catalog component by itself. The manifests for this component can be found in ~/environment/eks-workshop/base-application/catalog.\n$ ls ~/environment/eks-workshop/base-application/catalog configMap.yaml deployment.yaml kustomization.yaml namespace.yaml secrets.yaml service-mysql.yaml service.yaml serviceAccount.yaml statefulset-mysql.yaml These manifests include the Deployment for the catalog API:\napiVersion: apps/v1 kind: Deployment metadata: name: catalog labels: app.kubernetes.io/created-by: eks-workshop app.kubernetes.io/type: app spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: catalog app.kubernetes.io/instance: catalog app.kubernetes.io/component: service template: metadata: annotations: prometheus.io/path: /metrics prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app.kubernetes.io/name: catalog app.kubernetes.io/instance: catalog app.kubernetes.io/component: service app.kubernetes.io/created-by: eks-workshop spec: serviceAccountName: catalog securityContext: fsGroup: 1000 containers: - name: catalog env: - name: DB_USER valueFrom: secretKeyRef: name: catalog-db key: username - name: DB_PASSWORD valueFrom: secretKeyRef: name: catalog-db key: password envFrom: - configMapRef: name: catalog securityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 image: \u0026#34;public.ecr.aws/aws-containers/retail-store-sample-catalog:0.4.0\u0026#34; imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8080 protocol: TCP livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 3 readinessProbe: httpGet: path: /health port: 8080 successThreshold: 3 periodSeconds: 5 resources: limits: memory: 512Mi requests: cpu: 250m memory: 512Mi volumeMounts: - mountPath: /tmp name: tmp-volume volumes: - name: tmp-volume emptyDir: medium: Memory This Deployment expresses the desired state of the catalog API component:\nUse the public.ecr.aws/aws-containers/retail-store-sample-catalog container image Run a single replica Expose the container on port 8080 named http Run probes/healthchecks against the /health path Requests a specific amount of CPU and memory so the Kubernetes scheduler can place it on a node with enough available resources Apply labels to the Pods so other resources can refer to them The manifests also include the Service used by other components to access the catalog API:\napiVersion: v1\rkind: Service\rmetadata:\rname: catalog\rlabels:\rapp.kubernetes.io/created-by: eks-workshop\rspec:\rtype: ClusterIP\rports:\r- port: 80\rtargetPort: http\rprotocol: TCP\rname: http\rselector:\rapp.kubernetes.io/name: catalog\rapp.kubernetes.io/instance: catalog\rapp.kubernetes.io/component: service This Service:\nSelects catalog Pods using labels that match what we expressed in the Deployment above Exposes itself on port 80 Targets the http port exposed by the Deployment, which translates to port 8080 Let\u0026rsquo;s create the catalog component:\n$ kubectl apply -k ~/environment/eks-workshop/base-application/catalog namespace/catalog created serviceaccount/catalog created configmap/catalog created secret/catalog-db created service/catalog created service/catalog-mysql created deployment.apps/catalog created statefulset.apps/catalog-mysql created Now we\u0026rsquo;ll see a new Namespace:\n$ kubectl get namespaces -l app.kubernetes.io/created-by=eks-workshop NAME STATUS AGE catalog Active 15s We can take a look at the Pods running in this namespace:\n$ kubectl get pod -n catalog NAME READY STATUS RESTARTS AGE catalog-846479dcdd-fznf5 1/1 Running 2 (43s ago) 46s catalog-mysql-0 1/1 Running 0 46s Notice we have a Pod for our catalog API and another for the MySQL database. If the catalog Pod is showing a status of CrashLoopBackOff, it needs to be able to connect to the catalog-mysql Pod before it will start. Kubernetes will keep restarting it until this is the case. In that case, we can use kubectl wait to monitor specific Pods until they are in a Ready state:\n$ kubectl wait --for=condition=Ready pods --all -n catalog --timeout=180s Now that the Pods are running we can check their logs, for example the catalog API:\nYou can \u0026ldquo;follow\u0026rdquo; the kubectl logs output by using the \u0026lsquo;-f\u0026rsquo; option with the command. (Use CTRL-C to stop following the output)\n$ kubectl logs -n catalog deployment/catalog Kubernetes also allows us to easily scale the number of catalog Pods horizontally:\n$ kubectl scale -n catalog --replicas 3 deployment/catalog deployment.apps/catalog scaled $ kubectl wait --for=condition=Ready pods --all -n catalog --timeout=180s The manifests we applied also create a Service for each of our application and MySQL Pods that can be used by other components in the cluster to connect:\n$ kubectl get svc -n catalog NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE catalog ClusterIP 172.20.83.84 \u0026lt;none\u0026gt; 80/TCP 2m48s catalog-mysql ClusterIP 172.20.181.252 \u0026lt;none\u0026gt; 3306/TCP 2m48s These Services are internal to the cluster, so we cannot access them from the Internet or even the VPC. However, we can use exec to access an existing Pod in the EKS cluster to check the catalog API is working:\n$ kubectl -n catalog exec -it \\ deployment/catalog -- curl catalog.catalog.svc/catalogue | jq . You should receive back a JSON payload with product information. Congratulations, you\u0026rsquo;ve just deployed your first microservice to Kubernetes with EKS!\n"
},
{
	"uri": "//localhost:1313/1-introduce/1.4-kube-api-server/",
	"title": "Kube API Server",
	"tags": [],
	"description": "",
	"content": "In this section, we will talk about kube-apiserver in kubernetes\nKube-apiserver is the primary component in kubernetes. Kube-apiserver is responsible for authenticating, validating requests, retrieving and Updating data in ETCD key-value store. In fact kube-apiserver is the only component that interacts directly to the etcd datastore. The other components such as kube-scheduler, kube-controller-manager and kubelet uses the API-Server to update in the cluster in their respective areas. Installing kube-apiserver If you are bootstrapping kube-apiserver using kubeadm tool, then you don\u0026rsquo;t need to know this, but if you are setting up using the hardway then kube-apiserver is available as a binary in the kubernetes release page.\nFor example: You can downlaod the kube-apiserver v1.13.0 binary here kube-apiserver\n$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-apiserver View kube-apiserver - Kubeadm kubeadm deploys the kube-apiserver as a pod in kube-system namespace on the master node. $ kubectl get pods -n kube-system View kube-apiserver options - Kubeadm You can see the options with in the pod definition file located at /etc/kubernetes/manifests/kube-apiserver.yaml $ cat /etc/kubernetes/manifests/kube-apiserver.yaml View kube-apiserver options - Manual In a Non-kubeadm setup, you can inspect the options by viewing the kube-apiserver.service $ cat /etc/systemd/system/kube-apiserver.service You can also see the running process and effective options by listing the process on master node and searching for kube-apiserver. $ ps -aux | grep kube-apiserver K8s Reference Docs: Kube API Server Command Line Tools Reference Kubernetes Components Overview Kubernetes API Overview Accessing Kubernetes Cluster Accessing Kubernetes Cluster API "
},
{
	"uri": "//localhost:1313/4-kustomize/",
	"title": "Kustomize",
	"tags": [],
	"description": "",
	"content": "\rPrepare your environment for this section:\n$ prepare-environment Kustomize allows you to manage Kubernetes manifest files using declarative \u0026ldquo;kustomization\u0026rdquo; files. It provides the ability to express \u0026ldquo;base\u0026rdquo; manifests for your Kubernetes resources and then apply changes using composition, customization and easily making cross-cutting changes across many resources.\nFor example, take a look at the following manifest file for the checkout Deployment:\n~/environment/eks-workshop/base-application/checkout/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: checkout labels: app.kubernetes.io/created-by: eks-workshop app.kubernetes.io/type: app spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: checkout app.kubernetes.io/instance: checkout app.kubernetes.io/component: service template: metadata: annotations: prometheus.io/path: /metrics prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app.kubernetes.io/name: checkout app.kubernetes.io/instance: checkout app.kubernetes.io/component: service app.kubernetes.io/created-by: eks-workshop spec: serviceAccountName: checkout securityContext: fsGroup: 1000 containers: - name: checkout envFrom: - configMapRef: name: checkout securityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true image: \u0026#34;public.ecr.aws/aws-containers/retail-store-sample-checkout:0.4.0\u0026#34; imagePullPolicy: IfNotPresent ports: - name: http containerPort: 8080 protocol: TCP livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 3 resources: limits: memory: 512Mi requests: cpu: 250m memory: 512Mi volumeMounts: - mountPath: /tmp name: tmp-volume volumes: - name: tmp-volume emptyDir: medium: Memory This file has already been applied in the previous Getting Started lab, but let\u0026rsquo;s say we wanted to scale this component horizontally by updating the replicas field using Kustomize. Rather than manually updating this YAML file, we\u0026rsquo;ll use Kustomize to update the spec/replicas field from 1 to 3.\nTo do so, we\u0026rsquo;ll apply the following kustomization.\nThe first tab shows the kustomization we\u0026rsquo;re applying The second tab shows a preview of what the updated Deployment/checkout file looks like after the kustomization is applied Finally, the third tab shows just the diff of what has changed modules/introduction/kustomize/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: checkout spec: replicas: 3 Deployment/checkout\napiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/created-by: eks-workshop app.kubernetes.io/type: app name: checkout namespace: checkout spec: replicas: 3 selector: matchLabels: app.kubernetes.io/component: service app.kubernetes.io/instance: checkout app.kubernetes.io/name: checkout template: metadata: annotations: prometheus.io/path: /metrics prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app.kubernetes.io/component: service app.kubernetes.io/created-by: eks-workshop app.kubernetes.io/instance: checkout app.kubernetes.io/name: checkout spec: containers: - envFrom: - configMapRef: name: checkout image: public.ecr.aws/aws-containers/retail-store-sample-checkout:0.4.0 imagePullPolicy: IfNotPresent livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 3 name: checkout ports: - containerPort: 8080 name: http protocol: TCP resources: limits: memory: 512Mi requests: cpu: 250m memory: 512Mi securityContext: capabilities: drop: - ALL readOnlyRootFilesystem: true volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: fsGroup: 1000 serviceAccountName: checkout volumes: - emptyDir: medium: Memory name: tmp-volume You can generate the final Kubernetes YAML that applies this kustomization with the kubectl kustomize command, which invokes kustomize that is bundled with the kubectl CLI:\n$ kubectl kustomize ~/environment/eks-workshop/modules/introduction/kustomize This will generate a lot of YAML files, which represents the final manifests you can apply directly to Kubernetes. Let\u0026rsquo;s demonstrate this by piping the output from kustomize directly to kubectl apply:\n$ kubectl kustomize ~/environment/eks-workshop/modules/introduction/kustomize | kubectl apply -f - namespace/checkout unchanged serviceaccount/checkout unchanged configmap/checkout unchanged service/checkout unchanged service/checkout-redis unchanged deployment.apps/checkout configured deployment.apps/checkout-redis unchanged You\u0026rsquo;ll notice that a number of different checkout-related resources are \u0026ldquo;unchanged\u0026rdquo;, with the deployment.apps/checkout being \u0026ldquo;configured\u0026rdquo;. This is intentional — we only want to apply changes to the checkout deployment. This happens because running the previous command actually applied two files: the Kustomize deployment.yaml that we saw above, as well as the following kustomization.yaml file which matches all files in the ~/environment/eks-workshop/base-application/checkout folder. The patches field specifies the specific file to be patched:\n~/environment/eks-workshop/modules/introduction/kustomize/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../../base-application/checkout patches: - path: deployment.yaml To check that the number of replicas has been updated, run the following command:\n$ kubectl get pod -n checkout -l app.kubernetes.io/component=service NAME READY STATUS RESTARTS AGE checkout-585c9b45c7-c456l 1/1 Running 0 2m12s checkout-585c9b45c7-b2rrz 1/1 Running 0 2m12s checkout-585c9b45c7-xmx2t 1/1 Running 0 40m Instead of using the combination of kubectl kustomize and kubectl apply we can instead accomplish the same thing with kubectl apply -k \u0026lt;kustomization_directory\u0026gt; (note the -k flag instead of -f). This approach is used through this workshop to make it easier to apply changes to manifest files, while clearly surfacing the changes to be applied.\nLet\u0026rsquo;s try that:\n$ kubectl apply -k ~/environment/eks-workshop/modules/introduction/kustomize To reset the application manifests back to their initial state, you can simply apply the original set of manifests:\n$ kubectl apply -k ~/environment/eks-workshop/base-application Another pattern you will see used in some lab exercises looks like this:\n$ kubectl kustomize ~/environment/eks-workshop/base-application \\ | envsubst | kubectl apply -f- This uses envsubst to substitute environment variable placeholders in the Kubernetes manifest files with the actual values based on your particular environment. For example in some manifests we need to reference the EKS cluster name with $EKS_CLUSTER_NAME or the AWS region with $AWS_REGION.\nNow that you understand how Kustomize works, you can proceed to the Helm module or go directly to the Fundamentals module.\nTo learn more about Kustomize, you can refer to the official Kubernetes documentation.\n"
},
{
	"uri": "//localhost:1313/5-helm/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": "\rPrepare your environment for this section:\n$ prepare-environment Although we will primarily be interacting with kustomize in this workshop, there will be situations where Helm will be used to install certain packages in the EKS cluster. In this lab we give a brief introduction to Helm, and we\u0026rsquo;ll demonstrate how to use it to install a pre-packaged application.\nThis lab doesn\u0026rsquo;t not cover the authoring of Helm charts for your own workloads. For more information on this topic see this guide.\nHelm is a package manager for Kubernetes that helps you define, install, and upgrade Kubernetes applications. It uses a packaging format called charts, which contain all the necessary Kubernetes resource definitions to run an application. Helm simplifies the deployment and management of applications on Kubernetes clusters.\nHelm CLI The helm CLI tool is typically used in conjunction with a Kubernetes cluster to manage the deployment and lifecycle of applications. It provides a consistent and repeatable way to package, install, and manage applications on Kubernetes, making it easier to automate and standardize application deployments across different environments.\nThe CLI is already installed in our IDE:\n$ helm version Helm repositories A Helm repository is a centralized location where Helm charts are stored and managed, and allow users to easily discover, share, and install charts. They facilitate easy access to a wide range of pre-packaged applications and services for deployment on Kubernetes clusters.\nThe Bitnami Helm repository is a collection of Helm charts for deploying popular applications and tools on Kubernetes. Let\u0026rsquo;s add the bitnami repository to our Helm CLI:\n$ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm repo update Now we can search the repository for charts, for example the postgresql chart:\n$ helm search repo postgresql NAME CHART VERSION APP VERSION DESCRIPTION bitnami/postgresql X.X.X X.X.X PostgreSQL (Postgres) is an open source object-... [...] Installing a Helm chart Let\u0026rsquo;s install an NGINX server in our EKS cluster using the Helm chart we found above. When you install a chart using the Helm package manager, it creates a new release for that chart. Each release is tracked by Helm and can be upgraded, rolled back, or uninstalled independently from other releases.\n$ echo $NGINX_CHART_VERSION $ helm install nginx bitnami/nginx \\ --version $NGINX_CHART_VERSION \\ --namespace nginx --create-namespace --wait We can break this command down as follows:\nUse the install sub-command to instruct Helm to install a chart Name the release nginx Use the chart bitnami/nginx with the version $NGINX_CHART_VERSION Install the chart in the nginx namespace and create that namespace first Wait for pods in the release to get to a ready state Once the chart has installed we can list the releases in our EKS cluster:\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION nginx nginx 1 2024-06-11 03:58:39.862100855 +0000 UTC deployed nginx-X.X.X X.X.X We can also see NGINX running in the namespace we specified:\n$ kubectl get pod -n nginx NAME READY STATUS RESTARTS AGE nginx-55fbd7f494-zplwx 1/1 Running 0 119s Configuring chart options In the example above we installed the NGINX chart in its default configuration. Sometimes you\u0026rsquo;ll need to provide configuration values to charts during installation to modify the way the component behaves.\nThere are two common ways to provide values to charts during installation:\nCreate YAML files and pass them to Helm using the -f or --values flag Pass values using the --set flag followed by key=value pairs Let\u0026rsquo;s combine these methods to update our NGINX release. We\u0026rsquo;ll use this values.yaml file:\n~/environment/eks-workshop/modules/introduction/helm/values.yaml\npodLabels: team: team1 costCenter: org1 resources: requests: cpu: 250m memory: 256Mi This adds several custom Kubernetes labels to the NGINX pods, as well as setting some resource requests.\nWe\u0026rsquo;ll also add additional replicas using the --set flag:\n$ helm upgrade --install nginx bitnami/nginx \\ --version $NGINX_CHART_VERSION \\ --namespace nginx --create-namespace --wait \\ --set replicaCount=3 \\ --values ~/environment/eks-workshop/modules/introduction/helm/values.yaml List the releases:\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION nginx nginx 2 2024-06-11 04:13:53.862100855 +0000 UTC deployed nginx-X.X.X X.X.X You\u0026rsquo;ll notice that the revision column has updated to 2 as Helm has applied our updated configuration as a distinct revision. This would allow us to rollback to our previous configuration if necessary.\nYou can view the revision history of a given release like this:\n$ helm history nginx -n nginx REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Tue Jun 11 03:58:39 2024 superseded nginx-X.X.X X.X.X Install complete 2 Tue Jun 11 04:13:53 2024 deployed nginx-X.X.X X.X.X Upgrade complete To check that our changes have taken effect list the pods in the nginx namespace:\n$ kubectl get pods -n nginx NAME READY STATUS RESTARTS AGE nginx-55fbd7f494-4hz9b 1/1 Running 0 30s nginx-55fbd7f494-gkr2j 1/1 Running 0 30s nginx-55fbd7f494-zplwx 1/1 Running 0 5m You can see we now have 3 replicas of the NGINX pod running.\nRemoving releases We can similarly uninstall a release using the CLI:\n$ helm uninstall nginx --namespace nginx --wait This will delete all the resources created by the chart for that release from our EKS cluster.\n"
},
{
	"uri": "//localhost:1313/1-introduce/1.5-kube-controller-manager/",
	"title": "Kube Controller Manager",
	"tags": [],
	"description": "",
	"content": "In this section, we will take a look at kube-controller-manager.\nKube Controller Manager manages various controllers in kubernetes. In kubernetes terms, a controller is a process that continuously monitors the state of the components within the system and works towards bringing the whole system to the desired functioning state. Node Controller Responsible for monitoring the state of the Nodes and taking necessary actions to keep the application running. Replication Controller It is responsible for monitoring the status of replicasets and ensuring that the desired number of pods are available at all time within the set. Other Controllers There are many more such controllers available within kubernetes Installing Kube-Controller-Manager When you install kube-controller-manager the different controllers will get installed as well.\nDownload the kube-controller-manager binary from the kubernetes release page. For example: You can download kube-controller-manager v1.13.0 here kube-controller-manager\n$ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-controller-manager By default all controllers are enabled, but you can choose to enable specific one from kube-controller-manager.service $ cat /etc/systemd/system/kube-controller-manager.service View kube-controller-manager - kubeadm kubeadm deploys the kube-controller-manager as a pod in kube-system namespace $ kubectl get pods -n kube-system View kube-controller-manager options - kubeadm You can see the options within the pod located at /etc/kubernetes/manifests/kube-controller-manager.yaml $ cat /etc/kubernetes/manifests/kube-controller-manager.yaml View kube-controller-manager options - Manual In a non-kubeadm setup, you can inspect the options by viewing the kube-controller-manager.service $ cat /etc/systemd/system/kube-controller-manager.service You can also see the running process and effective options by listing the process on master node and searching for kube-controller-manager. $ ps -aux | grep kube-controller-manager K8s Reference Docs: Kubernetes Command Line Tools Reference - kube-controller-manager Kubernetes Concepts Overview - Components "
},
{
	"uri": "//localhost:1313/3-getting-started/5-other-components/",
	"title": "Other Components",
	"tags": [],
	"description": "",
	"content": "In this lab exercise, we\u0026rsquo;ll deploy the rest of the sample application efficiently using the power of Kustomize. The following kustomization file shows how you can reference other kustomizations and deploy multiple components together:\nmanifests/base-application/kustomization.yaml Notice that the catalog API is in this kustomization, didn\u0026rsquo;t we already deploy it? Because Kubernetes uses a declarative mechanism we can apply the manifests for the catalog API again and expect that because all of the resources are already created Kubernetes will take no action.\nApply this kustomization to our cluster to deploy the rest of the components:\n$ kubectl apply -k ~/environment/eks-workshop/base-application After this is complete we can use kubectl wait to make sure all the components have started before we proceed:\n$ kubectl wait --for=condition=Ready --timeout=180s pods \\ -l app.kubernetes.io/created-by=eks-workshop -A We\u0026rsquo;ll now have a Namespace for each of our application components:\n$ kubectl get namespaces -l app.kubernetes.io/created-by=eks-workshop NAME STATUS AGE assets Active 62s carts Active 62s catalog Active 7m17s checkout Active 62s orders Active 62s other Active 62s rabbitmq Active 62s ui Active 62s We can also see all of the Deployments created for the components:\n$ kubectl get deployment -l app.kubernetes.io/created-by=eks-workshop -A NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE assets assets 1/1 1 1 90s carts carts 1/1 1 1 90s carts carts-dynamodb 1/1 1 1 90s catalog catalog 1/1 1 1 7m46s checkout checkout 1/1 1 1 90s checkout checkout-redis 1/1 1 1 90s orders orders 1/1 1 1 90s orders orders-mysql 1/1 1 1 90s ui ui 1/1 1 1 90s The sample application is now deployed and ready to provide a foundation for us to use in the rest of the labs in this workshop!\nIf you want to understand more about Kustomize take a look at the optional module provided in this workshop.\n"
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Make sure you have run the respective clean up instructions for the mechanism you used to provision the lab EKS cluster before proceeding, as instructed in 2.1 or 2.2 (depending on whether you use eksctl or Terraform to deploy your cluster).\nAfter cleaning up the cluster, run the following command to remove the IDE stack:\naws cloudformation delete-stack --stack-name eks-workshop-ide "
},
{
	"uri": "//localhost:1313/1-introduce/1.6-kube-scheduler/",
	"title": "Kube Scheduler",
	"tags": [],
	"description": "",
	"content": "In this section, we will take a look at kube-scheduler.\nkube-scheduler is responsible for scheduling pods on nodes. The kube-scheduler is only responsible for deciding which pod goes on which node. It doesn\u0026rsquo;t actually place the pod on the nodes, that\u0026rsquo;s the job of the kubelet. Why do you need a Scheduler? kube-scheduler plays an important role in resource management and ensuring that pods are allocated efficiently across nodes. Install kube-scheduler - Manual Download the kubescheduler binary from the kubernetes release pages kube-scheduler. For example: To download kube-scheduler v1.13.0, Run the below command. $ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-scheduler Extract it Run it as a service View kube-scheduler options - kubeadm If you set it up with kubeadm tool, kubeadm tool will deploy the kube-scheduler as pod in kube-system namespace on master node. $ kubectl get pods -n kube-system You can see the options for kube-scheduler in pod definition file that is located at /etc/kubernetes/manifests/kube-scheduler.yaml $ cat /etc/kubernetes/manifests/kube-scheduler.yaml You can also see the running process and effective options by listing the process on master node and searching for kube-apiserver. $ ps -aux | grep kube-scheduler K8s Reference Docs:\nkube-scheduler CLI Scheduling và Eviction with kube-scheduler Components Configure multiple schedulers "
},
{
	"uri": "//localhost:1313/1-introduce/1.7-kubelet/",
	"title": "Kubelet",
	"tags": [],
	"description": "",
	"content": "In this section we will take a look at kubelet.\nKubelet is the sole point of contact for the kubernetes cluster The kubelet will create the pods on the nodes, the scheduler only decides which pods goes where. Install kubelet Kubeadm does not deploy kubelet by default. You must manually download and install it. Download the kubelet binary from the kubernetes release pages kubelet. For example: To download kubelet v1.13.0, Run the below command. $ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubelet Extract it Run it as a service View kubelet options You can also see the running process and affective options by listing the process on worker node and searching for kubelet. $ ps -aux |grep kubelet K8s Reference Docs: Command Line Tools Reference - Kubelet Overview Components Kubeadm Kubelet Integration "
},
{
	"uri": "//localhost:1313/1-introduce/1.8-kube-proxy/",
	"title": "kube-proxy",
	"tags": [],
	"description": "",
	"content": "In this section, we will take a look at kube-proxy.\nWithin Kubernetes Cluster, every pod can reach every other pod, this is accomplish by deploying a pod networking cluster to the cluster.\nKube-Proxy is a process that runs on each node in the kubernetes cluster. Install kube-proxy - Manual Download the kube-proxy binary from the kubernetes release pages kube-proxy. For example: To download kube-proxy v1.13.0, Run the below command. $ wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-proxy Extract it Run it as a service View kube-proxy options - kubeadm If you set it up with kubeadm tool, kubeadm tool will deploy the kube-proxy as pod in kube-system namespace. In fact it is deployed as a daemonset on master node. $ kubectl get pods -n kube-system K8s Reference Docs: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/ https://kubernetes.io/docs/concepts/overview/components/ "
},
{
	"uri": "//localhost:1313/1-introduce/1.9-pods/",
	"title": "Pods",
	"tags": [],
	"description": "",
	"content": "In the Kubernetes system, Pods are an important concept. Kubernetes does not deploy containers directly to worker nodes. Instead, Kubernetes uses Pods to manage containers.\nEach Pod in Kubernetes contains one or more containers, but typically they consist of a single container each, and that is the instance of the application you are running. Pods will have a one-to-one relationship with the containers running your application.\nMulti-Container PODs A single pod can have multiple containers except for the fact that they are usually not multiple containers of the same kind. How to deploy pods? Lets now take a look to create a nginx pod using kubectl.\nTo deploy a docker container by creating a POD. $ kubectl run nginx --image nginx To get the list of pods $ kubectl get pods K8s Reference Docs Kubernetes - Pod Kubernetes - Pod Overview Kubernetes - Explore Introduction "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]